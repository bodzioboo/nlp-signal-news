{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS918 - Assignment 1 - 1632704"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load the libraries needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import timeit\n",
    "import math\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time the code execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = timeit.default_timer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_news = []\n",
    "with open('signal-news1.jsonl','r',encoding = 'utf-8') as f:\n",
    "    for line in f:\n",
    "        signal_news.append(json.loads(line))\n",
    "del line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep only the data from the 'contents' section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_news = [signal_news[i]['content'] for i in range(0,len(signal_news)-1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Text Preprocressing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. After lowercasing all the text, use regular expressions to parse and clean the texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_news = [word.lower() for word in signal_news]\n",
    "signal_news = [word.replace('\\n','') for word in signal_news]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Remove URLs. Note that URLs may appear in different forms, e.g. “http://www.*”, “http://domain”, “https://www.*”:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_news = [re.sub(r'(?:https?\\:\\/\\/)?(?:www\\.)?[a-zA-Z0-9-]+\\.(?:(?:[a-zA-Z0-9-]+\\.)*)?[a-z]{2,4}(?:(?:\\/\\S+)*)?','',word) for word in signal_news]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logic of the URL regex goes as follows: \n",
    "\n",
    "(?:https?:\\/\\/)? - non capture group with 1 or 0 occurences for the Hyper Text Transfer Protocol tag\n",
    "\n",
    "(?:www\\.)? - noon capture group with 1 or 0 occurences for www\n",
    "\n",
    "[a-z0-9-]+\\. - first part of websites name\n",
    "\n",
    "(?:(?:[a-z0-9-]+\\.)*)? - optional capture group to capture multi-worded domain names separated by dot (e.g. search.google.com) or names with multiple top level domains (e.g. data.gov.uk)\n",
    "\n",
    "[a-z]{2,4} - final top level domain name, assumping it's no longer than 4 characters, but not shorter than 2 (which should usually be the case)\n",
    "\n",
    "(?:(?:\\/\\S+)*)? - any futher possible combinations of characters separated by slashes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **Remove all non-alphanumeric characters except spaces, i.e. keep only alphanumeric characters and spaces:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*1. Keep only alphanumeric - instance 1: for words such as 20-year-old, replace '-' with space:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_news = [re.sub(r'(?:([a-z0-9])\\-([a-z0-9]))+',r'\\1 \\2',word) for word in signal_news]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logic of the above regex is as follows:\n",
    "- Select at least one instance of a combination 'character-character' (for example '8-y' and 's-o' in '18-years-old')\n",
    "- relace content between two capture groups with a space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*2. Keep only alphanumeric for other cases - simply remove all non-alphanumeric characters:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_news = [re.sub(r'[^a-z0-9 ]','',word) for word in signal_news]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above regex substitutes all characters that aren't letters, numbers or spaces to ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**- Remove words with only 1 character:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_news = [re.sub(r'\\b(\\w)\\b','',word) for word in signal_news]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**- Remove numbers that are fully made of digits (e.g. you should remove the number ‘5’, but\n",
    "in the case of ‘5pm’, made of both digits and letters, you should keep it as is, without\n",
    "removing the digit that is part of the word):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_news = [re.sub(r'\\b\\d+\\b','',word) for word in signal_news]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final step, replace all the extra whitespaces created during pre-processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_news = [re.sub(r' +',' ',word) for word in signal_news]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Use an English lemmatiser to process all the words. Use of a POS tagger is optional, and you may instead assign each word the default POS tag when using the lemmatiser. [6 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*2.1. Tokenize each text:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "signal_news_tokenized = [nltk.word_tokenize(word) for word in signal_news]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*2.2. Lemmatize using default POS tagger:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "signal_news_lemmatized = signal_news_tokenized.copy()\n",
    "for i in range(len(signal_news_tokenized)-1):\n",
    "    signal_news_lemmatized[i] = [lemmatizer.lemmatize(text) for text in signal_news_tokenized[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: N-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Compute N (number of tokens) and V (vocabulary size):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of tokens is 5745921.\n",
      "The vocabulary size is 109420.\n"
     ]
    }
   ],
   "source": [
    "#merge all vocabulary into one list:\n",
    "signal_news_lemmatized_full = [word for text in signal_news_lemmatized for word in text]\n",
    "#Compute:\n",
    "print('The number of tokens is {}.'.format(len(signal_news_lemmatized_full)))\n",
    "print('The vocabulary size is {}.'.format(len(set(signal_news_lemmatized_full))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. List the top 25 trigrams based on the number of occurrences on the entire corpus:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*2.1. Define a function for counting ngrams in text:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_count(textdata,n):\n",
    "    #1. list of lists of trigrams in each text\n",
    "    textdata_trigrams = [list(nltk.ngrams(text,n)) for text in textdata]\n",
    "    #2. join all into one list\n",
    "    textdata_trigrams_all = []\n",
    "    for elem in textdata_trigrams:\n",
    "        textdata_trigrams_all.extend(elem)\n",
    "        #3. count each:\n",
    "    textdata_trigrams_count = {}\n",
    "    for elem in textdata_trigrams_all:\n",
    "        if elem not in textdata_trigrams_count:\n",
    "            textdata_trigrams_count[elem] = 1\n",
    "        else:\n",
    "            textdata_trigrams_count[elem] += 1\n",
    "    return textdata_trigrams_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*2.2. Count the trigrams using the function defined above and sort them in descending order by count*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Tirgram: ('one', 'of', 'the') occured 2430 times\n",
      "2. Tirgram: ('on', 'share', 'of') occured 2093 times\n",
      "3. Tirgram: ('day', 'moving', 'average') occured 1979 times\n",
      "4. Tirgram: ('on', 'the', 'stock') occured 1566 times\n",
      "5. Tirgram: ('a', 'well', 'a') occured 1420 times\n",
      "6. Tirgram: ('in', 'research', 'report') occured 1415 times\n",
      "7. Tirgram: ('in', 'research', 'note') occured 1373 times\n",
      "8. Tirgram: ('for', 'the', 'quarter') occured 1221 times\n",
      "9. Tirgram: ('the', 'united', 'state') occured 1218 times\n",
      "10. Tirgram: ('the', 'year', 'old') occured 1216 times\n",
      "11. Tirgram: ('average', 'price', 'of') occured 1193 times\n",
      "12. Tirgram: ('research', 'report', 'on') occured 1177 times\n",
      "13. Tirgram: ('research', 'note', 'on') occured 1138 times\n",
      "14. Tirgram: ('share', 'of', 'the') occured 1132 times\n",
      "15. Tirgram: ('the', 'end', 'of') occured 1129 times\n",
      "16. Tirgram: ('in', 'report', 'on') occured 1124 times\n",
      "17. Tirgram: ('earnings', 'per', 'share') occured 1121 times\n",
      "18. Tirgram: ('buy', 'rating', 'to') occured 1075 times\n",
      "19. Tirgram: ('cell', 'phone', 'plan') occured 1073 times\n",
      "20. Tirgram: ('phone', 'plan', 'detail') occured 1070 times\n",
      "21. Tirgram: ('of', 'the', 'company') occured 1057 times\n",
      "22. Tirgram: ('according', 'to', 'the') occured 1048 times\n",
      "23. Tirgram: ('appeared', 'first', 'on') occured 995 times\n",
      "24. Tirgram: ('moving', 'average', 'price') occured 995 times\n",
      "25. Tirgram: ('price', 'target', 'on') occured 981 times\n"
     ]
    }
   ],
   "source": [
    "signal_news_trigrams_count = ngram_count(signal_news_lemmatized,3)\n",
    "#Sort:\n",
    "signal_news_trigrams_count_sorted = sorted(signal_news_trigrams_count.items(),key = lambda x: x[1], reverse = True)\n",
    "#Get the top 25\n",
    "for i, (trigram,number) in enumerate(signal_news_trigrams_count_sorted[0:25]):\n",
    "    print('{}. Tirgram: {} occured {} times'.format(i+1,trigram,number))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Using the lists of positive and negative words provided with the corpus, compute the number of positive and negative word counts in the corpus:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*3.1 Create two sets, one containing the positive words and one containing the negative words:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'opinion-lexicon-English\\\\positive-words.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-b60f57f38b29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Create a set of positive words:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpositive_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'opinion-lexicon-English\\\\positive-words.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mpositive_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpositive_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n,'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpositive_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'opinion-lexicon-English\\\\positive-words.txt'"
     ]
    }
   ],
   "source": [
    "#Create a set of positive words:\n",
    "positive_words = []\n",
    "with open('opinion-lexicon-English\\\\positive-words.txt','r') as p:\n",
    "    positive_words.append(p.readlines())\n",
    "positive_words = set([word.strip('\\n,') for word in positive_words[0]])\n",
    "#Create a set of negative words:\n",
    "negative_words = []\n",
    "with open('opinion-lexicon-English\\\\negative-words.txt','r') as n:\n",
    "    negative_words.append(n.readlines())\n",
    "negative_words = set([word.strip('\\n,') for word in negative_words[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*3.2. Second, join all the separate tokenized corpus lists in one long list:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_news_lemmatized_all = []\n",
    "for elem in signal_news_lemmatized:\n",
    "    signal_news_lemmatized_all.extend(elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*3.3 Define a function for counting occurences:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define utlity function for that purpose\n",
    "#takes two arguments:\n",
    "#textlist - tokenized corpus passed as a list\n",
    "#wordset - set of words that are to be counted within the tokenized corpus\n",
    "def count_words(textlist,wordset):\n",
    "    words_count = {} #create empty dictionary to store occurences of each word\n",
    "    for elem in textlist: #iterate over elements in tokenized list from corpus\n",
    "        if elem not in words_count:\n",
    "            words_count[elem] = 1 #create key-value pair if the word wasn't counted\n",
    "        else:\n",
    "            words_count[elem] += 1 #add one to value if the word already in the dictionary\n",
    "    #sum the count of all words that are included in the wordset\n",
    "    word_total = sum([words_count[k] for k in wordset if k in words_count.keys()])\n",
    "    return word_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above function creates an empty dictionary and then populates it with words included in *wordset* that occured in *textlist* as keys and counts of their occurences in *textlist* as values. Then it returns total count of these words. The function is then applied to positive and negative word lexicons and the lematized corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of positive words in the corpus is 175361.\n",
      "The total number of negative words in the corpus is 132257.\n"
     ]
    }
   ],
   "source": [
    "print('The total number of positive words in the corpus is {}.'.format(count_words(signal_news_lemmatized_all,positive_words)))\n",
    "print('The total number of negative words in the corpus is {}.'.format(count_words(signal_news_lemmatized_all,negative_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Compute the number of news stories with more positive than negative words, as well as the number of news stories with more negative than positive words. News stories with a tie (same number of positive and negative words) should not be counted:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do that, the *count_words* function is used to create a comparison list - a list of length equal as the number of texts in the entire corpus, with positive values for more positive words and negative for more negative. Then the sum of positive and negative elements in the list is computed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10883 articles with more positive than negative words\n",
      "There are 6346 articles with more negative than positive words\n"
     ]
    }
   ],
   "source": [
    "words_comparison = [count_words(elem, positive_words)-count_words(elem,negative_words) for elem in signal_news_lemmatized]\n",
    "positive_article_count = len([k for k in words_comparison if k > 0])\n",
    "print('There are {} articles with more positive than negative words'.format(positive_article_count))\n",
    "negative_article_count = len([k for k in words_comparison if k < 0])\n",
    "print('There are {} articles with more negative than positive words'.format(negative_article_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C: Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data into training and test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_news_train = signal_news_lemmatized[:16000]\n",
    "signal_news_test = signal_news_lemmatized[16001:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the Language Model for trigram, *LanguageModel* class was created. It consists of:\n",
    "1. ***__init__ ***method, which initializes an instance of the class by assigning the training data and basic model as its main attributes.\n",
    "\n",
    "2. ***compute_model_trigrams*** method which uses two nested collections.defaultdict objects populated with 1s (for the purpose of Add-1 smoothing in preplexity calculation). Each bigram (w1,w2) appearing in the text is associated with frequencies of unigrams w3 occuring after it.\n",
    "\n",
    "3. ***compute_model_bigrams*** method used for back-off in preplexity calculation. Computed by analogy to the method above, only this time calculates frequencies of unigrams w2 appearing after each unigram w1 in the trainin data.\n",
    "\n",
    "4. ***compute_model_unigrams*** method used for back-off in preplexity calculation. By analogy to the two above, only this time calculates unigram model, i.e. frequencies of each word.\n",
    "\n",
    "5. ***generate_sentence*** method which takes two initial words and sequentially predicts *i* words occuring next by selecting those with highest conditional frequency from the trigram model.\n",
    "\n",
    "6. ***compute_denominators*** method, which calculates the denominators used for normalization of Maximum Likelihood Estimates of conditional word probabilities after applying add-1 smoothing. Denominator for each observed ngram is equal to the sum of frequencies associated with it + the vocabulary size for the ngram (possible words following it).\n",
    "\n",
    "7. ***compute_perplexity*** method, which, given a *test_set* performs intrinsic evaluation of the fitted model. It does so by assigning each word w3 following a bigram (w1,w2) in the unobserved data with log of probability predicted by the trained model. These probabilities are obtained by dividing the add-1 frequency by the normalized denominator computed by the *compute_denominator* method. In case of a bigram not being observed by the trigram model, it tries to use a bigram and then a unigram. As a last resort, it estimates the probability as 1 over the number of unique bigrams in the entire corpusus. Finally, the perplexity is given by $e^{-\\frac{1}{n}log(w_{1},w_{2},...,w_{n})}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel:\n",
    "    def __init__(self,train_data):\n",
    "        self.data = train_data\n",
    "        self.model_trigrams = self.compute_model_trigrams()\n",
    "        self.model_bigrams = self.compute_model_bigrams()\n",
    "        self.model_unigrams = self.compute_model_unigrams()\n",
    "\n",
    "        \n",
    "    def compute_model_trigrams(self):\n",
    "        model_trigrams = defaultdict(lambda:defaultdict(lambda:1))\n",
    "        for text in self.data:\n",
    "            for w1, w2, w3 in nltk.ngrams(text,3):\n",
    "                model_trigrams[(w1,w2)][w3] += 1\n",
    "        return model_trigrams\n",
    "    \n",
    "    def compute_model_bigrams(self):\n",
    "        model_bigrams = defaultdict(lambda:defaultdict(lambda:1))\n",
    "        for text in self.data:\n",
    "            for w1, w2 in nltk.ngrams(text,2):\n",
    "                model_bigrams[w1][w2] += 1\n",
    "        return model_bigrams\n",
    "    \n",
    "    def compute_model_unigrams(self):\n",
    "        model_unigrams = defaultdict(lambda:0)\n",
    "        for text in self.data:\n",
    "            for w1 in nltk.ngrams(text,1):\n",
    "                model_unigrams[w1] += 1\n",
    "        return model_unigrams\n",
    "    \n",
    "    def generate_sentence(self,word1,word2,i):\n",
    "        sentence = ''.join([word1, ' ',word2])\n",
    "        while i > 0:\n",
    "            words = dict(self.model_trigrams[word1,word2])\n",
    "            word1 = word2\n",
    "            word2 = max(words,key = words.get)\n",
    "            sentence = sentence + ' ' + word2\n",
    "            i -= 1\n",
    "        return(sentence)\n",
    "    \n",
    "    def compute_denominators(self,model):\n",
    "        denominators = dict()\n",
    "        for w in model:\n",
    "            denominator = int((sum(model[w].values())) + len(model[w]))\n",
    "            denominators[w] = denominator\n",
    "        return denominators\n",
    "\n",
    "    \n",
    "    def compute_perplexity(self,test_data):\n",
    "        denominators_trigrams = self.compute_denominators(self.model_trigrams)\n",
    "        denominators_bigrams = self.compute_denominators(self.model_bigrams)\n",
    "        denominators_unigrams = float(sum(self.model_unigrams.values()) + len(ngram_count(self.data,1))) #total count + count of unique unigrams\n",
    "        unique_bigrams = len(ngram_count(self.data,2))\n",
    "        trigrams_test = []\n",
    "        test_data_full = []\n",
    "        for text in test_data:\n",
    "            test_data_full.extend(text)\n",
    "        N = len(set(test_data_full))\n",
    "        for text in test_data:\n",
    "            trigram = [(w1,w2,w3) for w1,w2,w3 in nltk.ngrams(text,3)]\n",
    "            trigrams_test.append(trigram)\n",
    "        #set log_prob to 1\n",
    "        log_prob = 1\n",
    "        for trigram in trigrams_test:\n",
    "            for w1,w2,w3 in trigram:\n",
    "                #first - try to get probability from the smoothed trigram model:\n",
    "                if (w1,w2) in self.model_trigrams.keys():\n",
    "                    #ideally the if statement would be sufficient, however errors occured -> exception handling necessary\n",
    "                    try:\n",
    "                        log_prob += math.log(self.model_trigrams[(w1,w2)][w3]/denominators_trigrams[(w1,w2)])\n",
    "                    except:\n",
    "                        #if the bigram did not appear in the trigram model, try a bigram model for the second word:  \n",
    "                        if (w1,w2) in self.model_bigrams.keys():\n",
    "                            log_prob += math.log(self.model_bigrams[w2][w3]/denominators_bigrams[w2])\n",
    "                        #otherwise, try estimating using unigram\n",
    "                        else:\n",
    "                            try:\n",
    "                                log_prob += math.log(self.model_unigrams[w3]/denominators_unigrams)\n",
    "                            #last resort: estimate probability as 1/count of unique bigrams in the entire corpus\n",
    "                            except:\n",
    "                                log_prob += math.log(self.model_trigrams[(w1,w2)][w3]/unique_bigrams)\n",
    "        perplexity = math.e**(-(1/N)*log_prob)\n",
    "        return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code initializes the Language Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LanguageModel(signal_news_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Compute language models for trigrams based on the first 16,000 rows of the corpus. Beginning with the bigram “is this”, produce a sentence of 10 words by appending the most likely next word each time.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following line of code predicts a 10 word sentence beginning with a bigram ('is','this'):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'do something about it the first time in the first time in the first time in the first time in the first'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.generate_sentence('do','something',20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Compute the perplexity by evaluating on the remaining rows of the corpus (rows 16,001+).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.compute_perplexity(signal_news_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perplexity is extremely high, which points at poor performance of the created model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_end = timeit.default_timer()\n",
    "print(time_end-time_start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
